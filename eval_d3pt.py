import argparse
import csv
from datasets import load_dataset
import evaluate
from PIL import Image

# Import your project modules.
from image_processing import generate_caption_for_image, model as moondream_model
from qa_chain import CustomOllama
from utils import extract_thoughts

# For cosine similarity.
from sentence_transformers import SentenceTransformer, util
# For fuzzy matching.
from rapidfuzz import fuzz

# Initialize SentenceTransformer for cosine similarity.
embedder = SentenceTransformer('all-MiniLM-L6-v2')

def compute_cosine_similarity(prediction: str, reference: str) -> float:
    """Compute cosine similarity between the embeddings of prediction and reference."""
    emb_pred = embedder.encode(prediction, convert_to_tensor=True)
    emb_ref = embedder.encode(reference, convert_to_tensor=True)
    cos_sim = util.pytorch_cos_sim(emb_pred, emb_ref)
    return cos_sim.item()

def fuzzy_exact_match_presence(prediction: str, reference: str, threshold: int = 80) -> (bool, float):
    """
    Compute a fuzzy partial match score between the reference and prediction.
    Return a tuple (match_boolean, score). A score above the threshold is considered a match.
    """
    ratio = fuzz.partial_ratio(reference.lower(), prediction.lower())
    return ratio >= threshold, ratio

def main():
    parser = argparse.ArgumentParser(
        description=(
            "Evaluate DeepSeek (CustomOllama) with Moondream captioning. "
            "If the detailed caption is too short, a direct prompt is sent to Moondream "
            "to rewrite the caption with all relevant details to answer the question. "
            "The final answer is always generated by DeepSeek. "
            "For each response, cosine similarity, fuzzy exact match (with score), and METEOR score are printed and saved to CSV."
        )
    )
    parser.add_argument("--split", type=str, default="validation", help="Dataset split to evaluate (default: validation)")
    parser.add_argument("--temperature", type=float, default=0.6, help="Temperature for DeepSeek (default: 0.6)")
    parser.add_argument("--max_tokens", type=int, default=8192, help="Max tokens for DeepSeek responses (default: 8192)")
    parser.add_argument("--base_url", type=str, default="http://localhost:11434", help="Base URL for DeepSeek server")
    parser.add_argument("--limit", type=int, default=10, help="Limit the number of samples to evaluate (default: 100)")
    parser.add_argument("--caption_threshold", type=int, default=20, help="Minimum number of words required in the caption")
    parser.add_argument("--output_csv", type=str, default="evaluation_results.csv", help="CSV filename for saving the results")
    args = parser.parse_args()

    # Load the DocVQA dataset using the 'DocVQA' config.
    print("Loading DocVQA dataset...")
    dataset = load_dataset("lmms-lab/DocVQA", "DocVQA", split=args.split)
    dataset = dataset.select(range(min(args.limit, len(dataset))))
    print(f"Evaluating on {len(dataset)} samples.")

    # Initialize the DeepSeek model.
    llm = CustomOllama(
        model="deepseek-r1:70b",
        base_url=args.base_url,
        temperature=args.temperature,
        max_tokens=args.max_tokens
    )

    # Load the METEOR metric.
    meteor_metric = evaluate.load("meteor")

    results = []  # To store evaluation results for CSV.

    for i, sample in enumerate(dataset):
        print(f"\nProcessing sample {i+1}/{len(dataset)}")
        image = sample["image"]  # Expecting a PIL Image.
        question = sample["question"]
        ground_truth = sample["answers"][0] if isinstance(sample["answers"], list) else sample["answers"]

        # Step 1: Generate a detailed caption by default.
        caption = generate_caption_for_image(image, caption_length="detailed", debug=False)
        print("Initial Detailed Caption:", caption)

        # Step 2: If the caption is too short, directly prompt Moondream to rewrite it.
        if len(caption.split()) < args.caption_threshold:
            print(f"Caption is shorter than {args.caption_threshold} words.")
            requery_prompt = (
                f"Rewrite the image caption with more detail to fully answer the question: '{question}'. "
                f"Current caption: '{caption}'."
            )
            print("Direct requery prompt for Moondream:", requery_prompt)
            refined_result = moondream_model.query(image, requery_prompt)
            refined_caption = refined_result.get("answer", "")
            print("Refined Caption from Moondream:", refined_caption)
            caption = refined_caption

        # Step 3: Construct the final prompt for DeepSeek.
        final_prompt = f"Document Caption: {caption}\nQuestion: {question}\nAnswer:"
        print("Final Prompt for DeepSeek:", final_prompt)

        # Step 4: Get the final answer from DeepSeek.
        raw_answer = llm(final_prompt)
        _, main_answer = extract_thoughts(raw_answer)

        # Compute evaluation metrics.
        cos_sim = compute_cosine_similarity(main_answer, ground_truth)
        match_present, match_score = fuzzy_exact_match_presence(main_answer, ground_truth)
        meteor_result = meteor_metric.compute(predictions=[main_answer], references=[ground_truth])
        meteor_score = meteor_result.get("meteor", 0)

        # Print the evaluation results for this sample.
        print("Question           :", question)
        print("Ground Truth       :", ground_truth)
        print("Predicted Answer   :", main_answer)
        print(f"Cosine Similarity  : {cos_sim:.4f}")
        print(f"Fuzzy Exact Match  : {'Yes' if match_present else 'No'} (Score: {match_score:.2f})")
        print(f"METEOR Score       : {meteor_score:.4f}")
        print("-" * 80)

        # Append results to list.
        results.append({
            "sample_id": i + 1,
            "question": question,
            "ground_truth": ground_truth,
            "predicted_answer": main_answer,
            "cosine_similarity": cos_sim,
            "fuzzy_exact_match": "Yes" if match_present else "No",
            "fuzzy_match_score": match_score,
            "meteor_score": meteor_score
        })

    # Save results to CSV.
    csv_filename = args.output_csv
    with open(csv_filename, mode="w", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["sample_id", "question", "ground_truth", "predicted_answer", "cosine_similarity",
                      "fuzzy_exact_match", "fuzzy_match_score", "meteor_score"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        for result in results:
            writer.writerow(result)

    print(f"\nEvaluation results saved to {csv_filename}")

if __name__ == "__main__":
    main()
